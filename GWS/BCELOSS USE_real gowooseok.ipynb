{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:31:18.475822400Z",
     "start_time": "2024-03-27T03:31:18.398738300Z"
    }
   },
   "outputs": [],
   "source": [
    "### 모듈 로딩\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms \n",
    "from torchvision.datasets import ImageFolder \n",
    "### ===> Module Import\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset,DataLoader,WeightedRandomSampler\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from shutil import copy2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:31:18.496931Z",
     "start_time": "2024-03-27T03:31:18.478957600Z"
    }
   },
   "id": "229f5dfd9185a015",
   "execution_count": 545
  },
  {
   "cell_type": "markdown",
   "source": [
    "파일 전처리 함수 만들기"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a71b59557505227"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder \n",
    "\n",
    "img_root = \"./pizza_not_pizza/\" # 해당 경로 내에 있는 파일명이 곧 label이 되는 것\n",
    "train_root=\"./pizza_not_pizza/train\"\n",
    "test_root=\"./pizza_not_pizza/test\"\n",
    "validation_root=\"./pizza_not_pizza/validation\"\n",
    "\n",
    "\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "preprocessing = transforms.Compose([\n",
    "    transforms.Resize((227,227), interpolation=transforms.InterpolationMode.BILINEAR), # 1. resize\n",
    "    transforms.ToTensor(),  # 3. 값의 크기를 0~1로\n",
    "    transforms.Normalize(mean=mean, std=std) # 4. normalized\n",
    "])\n",
    "\n",
    "imgDS = ImageFolder(root=img_root, transform=preprocessing)\n",
    "validDS = ImageFolder(root=validation_root, transform=preprocessing)\n",
    "testDS = ImageFolder(root=test_root, transform=preprocessing)\n",
    "trainDS = ImageFolder(root=train_root, transform=preprocessing)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:31:18.542369200Z",
     "start_time": "2024-03-27T03:31:18.498993Z"
    }
   },
   "id": "5c590809b2d53f45",
   "execution_count": 546
  },
  {
   "cell_type": "markdown",
   "source": [
    "이미지 ds 클래스 확인, 클래스를 idx로 "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d3f78f1d7464102"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1966 1600 166 200\n"
     ]
    }
   ],
   "source": [
    "print(len(imgDS),len(trainDS),len(testDS),len(validDS))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:53:58.681760300Z",
     "start_time": "2024-03-27T03:53:58.654868900Z"
    }
   },
   "id": "1879fc9056974519",
   "execution_count": 569
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(['not_pizza', 'pizza'], {'not_pizza': 0, 'pizza': 1})"
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validDS.classes,validDS.class_to_idx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:31:18.570625800Z",
     "start_time": "2024-03-27T03:31:18.528738200Z"
    }
   },
   "id": "e5783ccdcc0255df",
   "execution_count": 547
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# weights = make_weights(imgDS.targets, len(imgDS.classes))\n",
    "# weights = torch.DoubleTensor(weights)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:31:18.605150500Z",
     "start_time": "2024-03-27T03:31:18.546490Z"
    }
   },
   "id": "7425e159360904a6",
   "execution_count": 548
  },
  {
   "cell_type": "markdown",
   "source": [
    "이미지 샘플러"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2064f674c194a6e9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# img_sampler = WeightedRandomSampler(weights, len(weights))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:31:18.609263800Z",
     "start_time": "2024-03-27T03:31:18.574778800Z"
    }
   },
   "id": "34c7aa0d4758d190",
   "execution_count": 549
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'permute'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[550], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m plt\u001B[38;5;241m.\u001B[39mimshow(\u001B[43mimgDS\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m400\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpermute\u001B[49m(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m0\u001B[39m))\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'int' object has no attribute 'permute'"
     ]
    }
   ],
   "source": [
    "plt.imshow(imgDS[400][1].permute(1,2,0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:31:18.697374200Z",
     "start_time": "2024-03-27T03:31:18.611421300Z"
    }
   },
   "id": "789e61889a70a45c",
   "execution_count": 550
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "trainDL = DataLoader(\n",
    "    trainDS, batch_size=batch_size, drop_last=True,shuffle=True\n",
    ")\n",
    "validDL = DataLoader(\n",
    "    validDS, batch_size=batch_size, drop_last=True,shuffle=True\n",
    ")\n",
    "\n",
    "testDL = DataLoader(\n",
    "    testDS, batch_size=batch_size, drop_last=True,shuffle=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:31:18.699625700Z",
     "start_time": "2024-03-27T03:31:18.654574Z"
    }
   },
   "id": "cc4a702ff489ffb6",
   "execution_count": 551
  },
  {
   "cell_type": "markdown",
   "source": [
    "[ 이미지랑 라벨 쪽 형태 보기]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "776d69649e0bab1e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.2.0  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "### ===> 딥러닝 모델을 설계할 때 활용하는 장비 확인\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:31:18.824254Z",
     "start_time": "2024-03-27T03:31:18.703752100Z"
    }
   },
   "id": "41d31fc160b5242b",
   "execution_count": 552
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:31:18.840322700Z",
     "start_time": "2024-03-27T03:31:18.760878400Z"
    }
   },
   "id": "7ff28ef028679ac1",
   "execution_count": 552
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(1966, 40.0, torch.utils.data.dataloader.DataLoader)"
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imgDS), len(trainDS)/40, type(trainDL) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:31:18.859511Z",
     "start_time": "2024-03-27T03:31:18.845671400Z"
    }
   },
   "id": "a5d60b8b59348fb3",
   "execution_count": 553
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n",
      "X_train: torch.Size([16, 3, 227, 227]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "### ===>  데이터 확인하기 (1)\n",
    "for (X_train, y_train) in trainDL:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:31:27.074883600Z",
     "start_time": "2024-03-27T03:31:18.864885600Z"
    }
   },
   "id": "20367458bfae61be",
   "execution_count": 554
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__() \n",
    "        self.conv1 = nn.Conv2d(in_channels =3, out_channels = 8, kernel_size = 3, padding = 1) \n",
    "        self.conv2 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 3, padding = 1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.fc1 = nn.Linear(\n",
    "            # 28*28*16,\n",
    "            50176,\n",
    "            128\n",
    "        )\n",
    "        self.fc2 =nn.Linear(128,64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) \n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x) \n",
    "        x = self.conv2(x) \n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x) \n",
    "        \n",
    "        # x = x.view(-1, 28*28*16)\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.fc4(x)\n",
    "        \n",
    "        return F.sigmoid(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:31:27.095681100Z",
     "start_time": "2024-03-27T03:31:27.083136200Z"
    }
   },
   "id": "7965546316b19332",
   "execution_count": 555
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=50176, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### ===> Optimizer, Objective Function 설정\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "model = CNN().to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001) #epoch마다 학습률을 0.001씩 줄임\n",
    "scheduler=ReduceLROnPlateau(optimizer,'min',patience=4)\n",
    "# criterion = nn.CrossEntropyLoss() # 손실함수\n",
    "# criterion = nn.BCELoss()\n",
    "criterion=nn.BCELoss()\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:31:27.157284200Z",
     "start_time": "2024-03-27T03:31:27.094683800Z"
    }
   },
   "id": "728581ad6554fb32",
   "execution_count": 556
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# CNN 모델 학습 진행 함수\n",
    "def train(epoch, model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        # print(image.shape, label.shape)\n",
    "        image = image.to(DEVICE)\n",
    "        # print(image.shape, label.shape)\n",
    "        \n",
    "        \n",
    "        label = label.to(DEVICE).unsqueeze(dim=1).float()\n",
    "        output = model(image)\n",
    "        # print(output.shape, label.shape)\n",
    "\n",
    "        \n",
    "        # print(output.shape)\n",
    "        # for p in model.parameters():\n",
    "        #     print(p)\n",
    "        # output_prob = F.sigmoid(output)\n",
    "        output_prob = output\n",
    "        # print(output_prob,label)\n",
    "        # print(output_prob.size(),label.size())\n",
    "        \n",
    "        # print(output_prob.shape, label.shape)\n",
    "        loss = criterion(output_prob, label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx}]\\tTrain Loss: {loss.item():.6f}\")\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:31:27.188516900Z",
     "start_time": "2024-03-27T03:31:27.159355600Z"
    }
   },
   "id": "e0239a6049afda42",
   "execution_count": 557
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "### 검증 validation 진행 함수 \n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            # print(image, label)\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE).unsqueeze(dim=1).float()\n",
    "            \n",
    "            \n",
    "            output = model(image)\n",
    "            output_prob = F.sigmoid(output)\n",
    "            #test_loss\n",
    "            test_loss += criterion(output_prob, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    test_loss /= len(testDL.dataset)\n",
    "    test_accuracy = 100. * correct / len(testDL.dataset)\n",
    "    return test_loss, test_accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:31:27.190592400Z",
     "start_time": "2024-03-27T03:31:27.174945400Z"
    }
   },
   "id": "b15bd87a855dba85",
   "execution_count": 558
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0]\tTrain Loss: 0.760439\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0439, \tTest Accuracy: 48.80 % \n",
      "\n",
      "Train Epoch: 2 [0]\tTrain Loss: 0.697082\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0439, \tTest Accuracy: 48.80 % \n",
      "Train Epoch: 3 [0]\tTrain Loss: 0.627490\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0434, \tTest Accuracy: 48.19 % \n",
      "\n",
      "Train Epoch: 4 [0]\tTrain Loss: 0.555552\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0433, \tTest Accuracy: 48.19 % \n",
      "Train Epoch: 5 [0]\tTrain Loss: 0.623226\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0428, \tTest Accuracy: 47.59 % \n",
      "Train Epoch: 6 [0]\tTrain Loss: 0.611180\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0434, \tTest Accuracy: 48.80 % \n",
      "Train Epoch: 7 [0]\tTrain Loss: 0.572760\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0428, \tTest Accuracy: 47.59 % \n",
      "Train Epoch: 8 [0]\tTrain Loss: 0.532921\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0437, \tTest Accuracy: 49.40 % \n",
      "Train Epoch: 9 [0]\tTrain Loss: 0.475795\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0433, \tTest Accuracy: 48.80 % \n",
      "Train Epoch: 10 [0]\tTrain Loss: 0.589183\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0435, \tTest Accuracy: 49.40 % \n",
      "Train Epoch: 11 [0]\tTrain Loss: 0.573320\n",
      "\n",
      "[EPOCH: 11], \tTest Loss: 0.0431, \tTest Accuracy: 48.19 % \n",
      "Train Epoch: 12 [0]\tTrain Loss: 0.432947\n",
      "\n",
      "[EPOCH: 12], \tTest Loss: 0.0435, \tTest Accuracy: 49.40 % \n",
      "Train Epoch: 13 [0]\tTrain Loss: 0.535775\n",
      "\n",
      "[EPOCH: 13], \tTest Loss: 0.0431, \tTest Accuracy: 48.19 % \n",
      "Train Epoch: 14 [0]\tTrain Loss: 0.685383\n",
      "\n",
      "[EPOCH: 14], \tTest Loss: 0.0434, \tTest Accuracy: 49.40 % \n",
      "Train Epoch: 15 [0]\tTrain Loss: 0.592970\n",
      "\n",
      "[EPOCH: 15], \tTest Loss: 0.0431, \tTest Accuracy: 48.19 % \n",
      "Train Epoch: 16 [0]\tTrain Loss: 0.573884\n",
      "\n",
      "[EPOCH: 16], \tTest Loss: 0.0431, \tTest Accuracy: 48.19 % \n",
      "Train Epoch: 17 [0]\tTrain Loss: 0.577153\n",
      "\n",
      "[EPOCH: 17], \tTest Loss: 0.0430, \tTest Accuracy: 48.19 % \n",
      "Train Epoch: 18 [0]\tTrain Loss: 0.568377\n",
      "\n",
      "[EPOCH: 18], \tTest Loss: 0.0433, \tTest Accuracy: 48.19 % \n",
      "Train Epoch: 19 [0]\tTrain Loss: 0.558746\n",
      "\n",
      "[EPOCH: 19], \tTest Loss: 0.0428, \tTest Accuracy: 47.59 % \n",
      "\n",
      "Train Epoch: 20 [0]\tTrain Loss: 0.584676\n",
      "\n",
      "[EPOCH: 20], \tTest Loss: 0.0433, \tTest Accuracy: 48.80 % \n"
     ]
    }
   ],
   "source": [
    "# CNN 학습\n",
    "EPOCHS =20\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    \n",
    "    train(epoch, model,trainDL, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, testDL)\n",
    "    \n",
    "    print(f\"\\n[EPOCH: {epoch}], \\tTest Loss: {test_loss:.4f}, \\tTest Accuracy: {test_accuracy:.2f} % \\n\")\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:46:13.633105200Z",
     "start_time": "2024-03-27T03:31:27.188516900Z"
    }
   },
   "id": "d27c31bac3db01f0",
   "execution_count": 559
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ALEXNET 사용"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bd93230f302f168"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "63fc0277a28fe7ef"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('features', Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (7): ReLU(inplace=True)\n",
      "  (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (9): ReLU(inplace=True)\n",
      "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "))\n",
      "('avgpool', AdaptiveAvgPool2d(output_size=(6, 6)))\n",
      "('classifier', Sequential(\n",
      "  (0): Dropout(p=0.5, inplace=False)\n",
      "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): Dropout(p=0.5, inplace=False)\n",
      "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): Linear(in_features=4096, out_features=1, bias=True)\n",
      "))\n",
      "Train Epoch: 1 [0]\tTrain Loss: 0.694700\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0249, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 2 [0]\tTrain Loss: 0.693468\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0250, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 3 [0]\tTrain Loss: 0.694314\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0250, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 4 [0]\tTrain Loss: 0.692998\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0249, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 5 [0]\tTrain Loss: 0.693359\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0250, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 6 [0]\tTrain Loss: 0.693233\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0250, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 7 [0]\tTrain Loss: 0.693358\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0250, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 8 [0]\tTrain Loss: 0.693475\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0250, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 9 [0]\tTrain Loss: 0.692802\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0249, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 10 [0]\tTrain Loss: 0.696332\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0249, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 11 [0]\tTrain Loss: 0.693727\n",
      "\n",
      "[EPOCH: 11], \tTest Loss: 0.0249, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 12 [0]\tTrain Loss: 0.692843\n",
      "\n",
      "[EPOCH: 12], \tTest Loss: 0.0250, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 13 [0]\tTrain Loss: 0.692564\n",
      "\n",
      "[EPOCH: 13], \tTest Loss: 0.0249, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 14 [0]\tTrain Loss: 0.692952\n",
      "\n",
      "[EPOCH: 14], \tTest Loss: 0.0249, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 15 [0]\tTrain Loss: 0.693158\n",
      "\n",
      "[EPOCH: 15], \tTest Loss: 0.0249, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 16 [0]\tTrain Loss: 0.692628\n",
      "\n",
      "[EPOCH: 16], \tTest Loss: 0.0249, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 17 [0]\tTrain Loss: 0.693492\n",
      "\n",
      "[EPOCH: 17], \tTest Loss: 0.0249, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 18 [0]\tTrain Loss: 0.692662\n",
      "\n",
      "[EPOCH: 18], \tTest Loss: 0.0250, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 19 [0]\tTrain Loss: 0.692708\n",
      "\n",
      "[EPOCH: 19], \tTest Loss: 0.0250, \tTest Accuracy: 50.00 % \n",
      "Train Epoch: 20 [0]\tTrain Loss: 0.692594\n",
      "\n",
      "[EPOCH: 20], \tTest Loss: 0.0249, \tTest Accuracy: 50.00 % \n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18,ResNet18_Weights,AlexNet\n",
    "\n",
    "\n",
    "#데이터 모델 학습 준비 \n",
    "model = AlexNet(num_classes=1)# 클래스 수에 맞춰 출력 레이어 수정(클래스 수 1개로)\n",
    "# model.classifier.append(nn.Linear(2,1))\n",
    "model.__dir__()\n",
    "for p in model.named_children():\n",
    "    print(p)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# 데이터 재설정 \n",
    "\n",
    "ALEXPREPRO = transforms.Compose([\n",
    "    transforms.Resize(256),  # 이미지를 리사이징\n",
    "    transforms.CenterCrop(224),  # 이미지 중심부 자르기\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# 데이터셋과 데이터로더 생성\n",
    "ALEXvalidDS = ImageFolder(root=validation_root, transform=ALEXPREPRO)\n",
    "ALEXtestDS = ImageFolder(root=test_root, transform=ALEXPREPRO)\n",
    "ALEXtrainDS = ImageFolder(root=train_root, transform=ALEXPREPRO)\n",
    "\n",
    "ALEXtrainDL = DataLoader(ALEXtrainDS, batch_size=32, shuffle=True)\n",
    "ALEXvalidDL = DataLoader(ALEXvalidDS, batch_size=32, shuffle=False)\n",
    "ALEXtestDL = DataLoader(ALEXtestDS, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# CNN 학습\n",
    "EPOCHS =20\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    train(epoch, model, ALEXtrainDL, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, ALEXtestDL)\n",
    "\n",
    "    print(f\"\\n[EPOCH: {epoch}], \\tTest Loss: {test_loss:.4f}, \\tTest Accuracy: {test_accuracy:.2f} % \\n\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T05:42:39.449653300Z",
     "start_time": "2024-03-27T05:10:08.634188500Z"
    }
   },
   "id": "1de77fd4eef50cfe",
   "execution_count": 570
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# summary(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:46:13.666137300Z",
     "start_time": "2024-03-27T03:46:13.654564300Z"
    }
   },
   "id": "27125adc07f62f5",
   "execution_count": 561
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# for p in model.named_parameters():\n",
    "#     print(p)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:46:13.680996900Z",
     "start_time": "2024-03-27T03:46:13.669720700Z"
    }
   },
   "id": "dacfb5b3e5b56d0f",
   "execution_count": 562
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# model.__dir__()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:46:13.696872300Z",
     "start_time": "2024-03-27T03:46:13.686400100Z"
    }
   },
   "id": "675a99aa0ba357d6",
   "execution_count": 563
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# for p in model.named_children():\n",
    "#     print(p)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:46:13.711762700Z",
     "start_time": "2024-03-27T03:46:13.699262Z"
    }
   },
   "id": "a5554a750221d9b1",
   "execution_count": 564
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# model.classifier[-1].out_features = 1\n",
    "# model.classifier[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:46:13.728258500Z",
     "start_time": "2024-03-27T03:46:13.717162400Z"
    }
   },
   "id": "f7986098084a2bdd",
   "execution_count": 565
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:46:13.743529300Z",
     "start_time": "2024-03-27T03:46:13.730356900Z"
    }
   },
   "id": "3bdc652f982a5b79",
   "execution_count": 565
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 모델, optimizer 상태도 저장\n",
    "torch.save(model.state_dict(), 'model')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:46:13.805962500Z",
     "start_time": "2024-03-27T03:46:13.752157Z"
    }
   },
   "id": "e38e8d2ab668cac6",
   "execution_count": 566
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T03:46:13.819882100Z",
     "start_time": "2024-03-27T03:46:13.808284800Z"
    }
   },
   "id": "94ef0456f162003",
   "execution_count": 566
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
